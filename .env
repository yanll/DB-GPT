#*******************************************************************#
#**             DB-GPT  - GENERAL SETTINGS                        **#  
#*******************************************************************#
## DISABLED_COMMAND_CATEGORIES - The list of categories of commands that are disabled. Each of the below are an option:
## pilot.commands.query_execute

## For example, to disable coding related features, uncomment the next line
# DISABLED_COMMAND_CATEGORIES=   

#*******************************************************************#
#**                        Webserver Port                         **#
#*******************************************************************#
# DBGPT_WEBSERVER_PORT=5670

#*******************************************************************#
#***                       LLM PROVIDER                          ***#
#*******************************************************************#

# TEMPERATURE=0

#*******************************************************************#
#**                         LLM MODELS                            **#
#*******************************************************************#
# LLM_MODEL, see dbgpt/configs/model_config.LLM_MODEL_CONFIG
#LLM_MODEL=vicuna-13b-v1.5
LLM_MODEL=proxyllm
## LLM model path, by default, DB-GPT will read the model path from LLM_MODEL_CONFIG based on the LLM_MODEL.
## Of course you can specify your model path according to LLM_MODEL_PATH
## In DB-GPT, the priority from high to low to read model path:
##    1. environment variable with key: {LLM_MODEL}_MODEL_PATH (Avoid multi-model conflicts)
##    2. environment variable with key: MODEL_PATH
##    3. environment variable with key: LLM_MODEL_PATH
##    4. the config in dbgpt/configs/model_config.LLM_MODEL_CONFIG
# LLM_MODEL_PATH=/app/models/vicuna-13b-v1.5
# LLM_PROMPT_TEMPLATE=vicuna_v1.1
MODEL_SERVER=http://127.0.0.1:8000
LIMIT_MODEL_CONCURRENCY=5
MAX_POSITION_EMBEDDINGS=4096
QUANTIZE_QLORA=True
QUANTIZE_8bit=True
# QUANTIZE_4bit=False
## SMART_LLM_MODEL - Smart language model (Default: vicuna-13b)
## FAST_LLM_MODEL - Fast language model (Default: chatglm-6b)
# SMART_LLM_MODEL=vicuna-13b
# FAST_LLM_MODEL=chatglm-6b
## Proxy llm backend, this configuration is only valid when "LLM_MODEL=proxyllm", When we use the rest API provided by deployment frameworks like fastchat as a proxyllm,
## "PROXYLLM_BACKEND" is the model they actually deploy. We can use "PROXYLLM_BACKEND" to load the prompt of the corresponding scene.
# PROXYLLM_BACKEND=

### You can configure parameters for a specific model with {model name}_{config key}=xxx
### See dbgpt/model/parameter.py
## prompt template for current model
# llama_cpp_prompt_template=vicuna_v1.1
## llama-2-70b must be 8
# llama_cpp_n_gqa=8
## Model path
# llama_cpp_model_path=/data/models/TheBloke/vicuna-13B-v1.5-GGUF/vicuna-13b-v1.5.Q4_K_M.gguf

### LLM cache
## Enable Model cache
MODEL_CACHE_ENABLE=True
## The storage type of model cache, now supports: memory, disk
MODEL_CACHE_STORAGE_TYPE=disk
## The max cache data in memory, we always store cache data in memory fist for high speed.
MODEL_CACHE_MAX_MEMORY_MB=2048
## The dir to save cache data, this configuration is only valid when MODEL_CACHE_STORAGE_TYPE=disk
## The default dir is pilot/data/model_cache
MODEL_CACHE_STORAGE_DISK_DIR=../../../tmp/dbgpt-cache

#*******************************************************************#
#**                         EMBEDDING SETTINGS                    **#
#*******************************************************************#
EMBEDDING_MODEL=text2vec
#EMBEDDING_MODEL=m3e-large
#EMBEDDING_MODEL=bge-large-en
#EMBEDDING_MODEL=bge-large-zh
KNOWLEDGE_CHUNK_SIZE=500
KNOWLEDGE_SEARCH_TOP_SIZE=5
## Maximum number of chunks to load at once, if your single document is too large,
## you can set this value to a higher value for better performance.
## if out of memory when load large document, you can set this value to a lower value.
# KNOWLEDGE_MAX_CHUNKS_ONCE_LOAD=10
#KNOWLEDGE_CHUNK_OVERLAP=50
# Control whether to display the source document of knowledge on the front end.
KNOWLEDGE_CHAT_SHOW_RELATIONS=False
# Whether to enable Chat Knowledge Search Rewrite Mode
KNOWLEDGE_SEARCH_REWRITE=False
## EMBEDDING_TOKENIZER   - Tokenizer to use for chunking large inputs
## EMBEDDING_TOKEN_LIMIT - Chunk size limit for large inputs
# EMBEDDING_MODEL=all-MiniLM-L6-v2
# EMBEDDING_TOKENIZER=all-MiniLM-L6-v2
# EMBEDDING_TOKEN_LIMIT=8191

## Openai embedding model, See dbgpt/model/parameter.py
# EMBEDDING_MODEL=proxy_openai
# proxy_openai_proxy_server_url=https://api.openai.com/v1
# proxy_openai_proxy_api_key={your-openai-sk}
# proxy_openai_proxy_backend=text-embedding-ada-002

## Common HTTP embedding model
# EMBEDDING_MODEL=proxy_http_openapi
# proxy_http_openapi_proxy_server_url=http://localhost:8100/api/v1/embeddings
# proxy_http_openapi_proxy_api_key=1dce29a6d66b4e2dbfec67044edbb924
# proxy_http_openapi_proxy_backend=text2vec



#*******************************************************************#
#**                  DB-GPT METADATA DATABASE SETTINGS            **#
#*******************************************************************#
### SQLite database (Current default database)
#LOCAL_DB_TYPE=sqlite

### MYSQL database
# LOCAL_DB_TYPE=mysql
# LOCAL_DB_USER=root
# LOCAL_DB_PASSWORD={your_password}
# LOCAL_DB_HOST=127.0.0.1
# LOCAL_DB_PORT=3306
# LOCAL_DB_NAME=dbgpt
### This option determines the storage location of conversation records. The default is not configured to the old version of duckdb. It can be optionally db or file (if the value is db, the database configured by LOCAL_DB will be used)
#CHAT_HISTORY_STORE_TYPE=db

#*******************************************************************#
#**                         COMMANDS                              **#
#*******************************************************************#
EXECUTE_LOCAL_COMMANDS=False



#*******************************************************************#
#**                  ALLOWLISTED PLUGINS                          **#
#*******************************************************************#

#ALLOWLISTED_PLUGINS - Sets the listed plugins that are allowed (Example: plugin1,plugin2,plugin3)
#DENYLISTED_PLUGINS - Sets the listed plugins that are not allowed (Example: plugin1,plugin2,plugin3)
ALLOWLISTED_PLUGINS=AutoGPTSearchEngine
DENYLISTED_PLUGINS=


#*******************************************************************#
#**                 CHAT PLUGIN SETTINGS                          **#
#*******************************************************************#
# CHAT_MESSAGES_ENABLED - Enable chat messages (Default: False)
# CHAT_MESSAGES_ENABLED=False


#*******************************************************************#
#**                  VECTOR STORE SETTINGS                       **#
#*******************************************************************#
### Chroma vector db config
VECTOR_STORE_TYPE=Chroma
#CHROMA_PERSIST_PATH=/root/DB-GPT/pilot/data

### Milvus vector db config
#VECTOR_STORE_TYPE=Milvus
#MILVUS_URL=127.0.0.1
#MILVUS_PORT=19530
#MILVUS_USERNAME
#MILVUS_PASSWORD
#MILVUS_SECURE=

### Weaviate vector db config
#VECTOR_STORE_TYPE=Weaviate
#WEAVIATE_URL=https://kt-region-m8hcy0wc.weaviate.network

#*******************************************************************#
#**                  WebServer Language Support                   **#
#*******************************************************************#
# en, zh, fr, ja, ko, ru
#LANGUAGE=en
LANGUAGE=zh


#*******************************************************************#
# **    PROXY_SERVER (openai interface | chatGPT proxy service), use chatGPT as your LLM.
# ** if your server can visit openai, please set PROXY_SERVER_URL=https://api.openai.com/v1/chat/completions
# ** else if you have a chatgpt proxy server, you can set PROXY_SERVER_URL={your-proxy-serverip:port/xxx}
#*******************************************************************#
API_AZURE_DEPLOYMENT=YPgpt
AZURE_OPENAI_ENDPOINT=https://yp2401.openai.azure.com
AZURE_OPENAI_KEY=2fcbec6687ec42b481bede40fbfcca15
PROXY_API_KEY=2fcbec6687ec42b481bede40fbfcca15
PROXY_SERVER_URL=https://yp2401.openai.azure.com
OPENAI_API_TYPE=azure
PROXY_API_VERSION=2024-02-15-preview
PROXY_API_BASE=https://yp2401.openai.azure.com
PROXY_API_TYPE=azure

#*******************************************************************#
# **  PROXY_SERVER +                                              **#
#*******************************************************************#

# Aliyun tongyi
#TONGYI_PROXY_API_KEY={your-tongyi-sk}

## Baidu wenxin
#WEN_XIN_MODEL_VERSION={version}
#WEN_XIN_API_KEY={your-wenxin-sk}
#WEN_XIN_API_SECRET={your-wenxin-sct}

## Zhipu
#ZHIPU_MODEL_VERSION={version}
#ZHIPU_PROXY_API_KEY={your-zhipu-sk}

## Baichuan
#BAICHUN_MODEL_NAME={version}
#BAICHUAN_PROXY_API_KEY={your-baichuan-sk}
#BAICHUAN_PROXY_API_SECRET={your-baichuan-sct}

# Xunfei Spark
#XUNFEI_SPARK_API_VERSION={version}
#XUNFEI_SPARK_APPID={your_app_id}
#XUNFEI_SPARK_API_KEY={your_api_key}
#XUNFEI_SPARK_API_SECRET={your_api_secret}

## Yi Proxyllm, https://platform.lingyiwanwu.com/docs
#YI_MODEL_VERSION=yi-34b-chat-0205
#YI_API_BASE=https://api.lingyiwanwu.com/v1
#YI_API_KEY={your-yi-api-key}

## Moonshot Proxyllm, https://platform.moonshot.cn/docs/
# MOONSHOT_MODEL_VERSION=moonshot-v1-8k
# MOONSHOT_API_BASE=https://api.moonshot.cn/v1
# MOONSHOT_API_KEY={your-moonshot-api-key}


#*******************************************************************#
#**    SUMMARY_CONFIG                                             **#
#*******************************************************************#
SUMMARY_CONFIG=FAST

#*******************************************************************#
#**    MUlti-GPU                                                  **#
#*******************************************************************#
## See https://developer.nvidia.com/blog/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/
## If CUDA_VISIBLE_DEVICES is not configured, all available gpus will be used
# CUDA_VISIBLE_DEVICES=0
## You can configure the maximum memory used by each GPU.
# MAX_GPU_MEMORY=16Gib

#*******************************************************************#
#**                         LOG                                   **#
#*******************************************************************#
# FATAL, ERROR, WARNING, WARNING, INFO, DEBUG, NOTSET
DBGPT_LOG_LEVEL=INFO
# LOG dir, default: ./logs
DBGPT_LOG_DIR=../../../tmp/logs


#*******************************************************************#
#**                         API_KEYS                              **#
#*******************************************************************#
# API_KEYS - The list of API keys that are allowed to access the API. Each of the below are an option, separated by commas.
# API_KEYS=dbgpt




BAIDU_COOKIE=BIDUPSID=DFC98451623604BBC2FEA8373D15141B; PSTM=1686011446; BAIDUID=DFC98451623604BBC6185F440267550B:FG=1; BD_UPN=123253; BDUSS=1pZnU5bHlMRGRxZ25YUFhxQ2Z1TEZicFRhMEw4TkFjR243RFczV0NnWE9LQmxtRUFBQUFBJCQAAAAAAAAAAAEAAAAKVTGTAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM6b8WXOm~FlbU; BDUSS_BFESS=1pZnU5bHlMRGRxZ25YUFhxQ2Z1TEZicFRhMEw4TkFjR243RFczV0NnWE9LQmxtRUFBQUFBJCQAAAAAAAAAAAEAAAAKVTGTAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM6b8WXOm~FlbU; SIGNIN_UC=70a2711cf1d3d9b1a82d2f87d633bd8a04603474344bbP3%2BorJb0znHQsxk2lsg6DmfkvYJz8hc8YXdSt9hIxsD9nnO8jtUde3f82HCNvp1Sz%2F%2Bgs8nVSNU6jLNmhGza%2BJFMh7CNobK4v2vpUlFj0Mod6mUbKTBY1i8KGg6kd41Vrv1bpNme%2Fo%2B6y1Xfkk%2FxpMh4Qmonhqwe6pQ%2FitdDGjjVtHju8JUhDywkRM1Uex%2Fl3aqNzusOkUXxkFo6IB5Cc7eco360Ugh%2Bop%2FF%2Be7YMle30f9RBIxmMFeGn6aMc1iLxyRh6ZivHdaAmDzflhWV1Q4ry0hWP7EBJhekFxW5s%3D86387401871677657723244008942178; newlogin=1; H_WISE_SIDS=40169_40212_40080_40365_40351_40301_40378_40334_40397_40415_40312_40446_40465_40460_40317_39662_40510_40514; MCITY=-131%3A; H_PS_PSSID=40169_40080_40301_40378_40334_40397_40415_40312_40446_40465_39662_40510_40514_60037_60046; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; BDSFRCVID=ezLOJeC62GvZd7otWiK6u9Zdre3SOxnTH6aouNgu5y_Vz-mMvHDKEG0PLf8g0KubrqyqogKK0mOTHUkF_2uxOjjg8UtVJeC6EG0Ptf8g0f5; H_BDCLCKID_SF=tbC8VCDKJKD3qbjkq45HMt00qxby26nHLen9aJ5y-J7nhpC9h-Ta0btTyp_8Xt3HL2oa-Rb-QpbZql5FQP-53R0h0PJkWp5k34jaKl0MLPb5hj6gQJoDj4TyDMnMBMPjamOnaU5o3fAKftnOM46JehL3346-35543bRTLnLy5KJYMDF4jTuKe5JWjNRabK6aKC5bL6rJabC3VPOJXU6q2bDeQNba36vN-GIHLpTafxcfqR5oyT3JXp0vWq54WbbvLT7johRTWqR4sxJgXUonDh83ebrWJxQAKHnnop6O5hvvhb6O3M7lMUKmDloOW-TB5bbPLUQF5l8-sq0x0bOte-bQXH_EJj-8tJ-e_K0QKt8_HRjYbb__-P4DeNjqWURZ56bHWh0bKqDWV4jaDx6v5jFlbh5-BMPj52OnKUT-3l7boMJRK5bdQUIT34jTKUR43bRTLInw2KjpOJo2353YhP-UyN3LWh37QC3lMKoaMp78jR093JO4y4Ldj4oxJpOJ5JbMopCafD_bbD_9DTtbePDyqx5Ka43tHD7yWCvjWhOcOR59K4nnDPDFeb5DL4oqL6I8hJb-5lvvhb3O3MOZKxLg5n7Tbb8eBgvZ2UQILnbcsq0x0b8WWtnLQtraLp0jBCOMahkMal7xOM5cQlPK5JkgMx6MqpQJQeQ-5KQN3KJmfbL9bT3YjjTyDautJ68Otn3K3-bVbJTMJjrIbtr5MbQH-UIsKRTrB2Q-5KL-3-QAeK3v5hJEyTK0eJ6bbtLeL2OOWfbdJJjofqrT5joBBpKhqfcu-fvRW2TxoUJ15DnJhhvG-R5a2J_ebPRiJPr9QgbPBhQ7tt5W8ncFbT7l5hKpbt-q0x-jLTnhVn0M5DK0HPonHjLaD6vX3J; BDRCVFR[feWj1Vr5u3D]=I67x6TjHwwYf0; delPer=0; BD_CK_SAM=1; PSINO=1; BAIDUID_BFESS=DFC98451623604BBC6185F440267550B:FG=1; BDSFRCVID_BFESS=ezLOJeC62GvZd7otWiK6u9Zdre3SOxnTH6aouNgu5y_Vz-mMvHDKEG0PLf8g0KubrqyqogKK0mOTHUkF_2uxOjjg8UtVJeC6EG0Ptf8g0f5; H_BDCLCKID_SF_BFESS=tbC8VCDKJKD3qbjkq45HMt00qxby26nHLen9aJ5y-J7nhpC9h-Ta0btTyp_8Xt3HL2oa-Rb-QpbZql5FQP-53R0h0PJkWp5k34jaKl0MLPb5hj6gQJoDj4TyDMnMBMPjamOnaU5o3fAKftnOM46JehL3346-35543bRTLnLy5KJYMDF4jTuKe5JWjNRabK6aKC5bL6rJabC3VPOJXU6q2bDeQNba36vN-GIHLpTafxcfqR5oyT3JXp0vWq54WbbvLT7johRTWqR4sxJgXUonDh83ebrWJxQAKHnnop6O5hvvhb6O3M7lMUKmDloOW-TB5bbPLUQF5l8-sq0x0bOte-bQXH_EJj-8tJ-e_K0QKt8_HRjYbb__-P4DeNjqWURZ56bHWh0bKqDWV4jaDx6v5jFlbh5-BMPj52OnKUT-3l7boMJRK5bdQUIT34jTKUR43bRTLInw2KjpOJo2353YhP-UyN3LWh37QC3lMKoaMp78jR093JO4y4Ldj4oxJpOJ5JbMopCafD_bbD_9DTtbePDyqx5Ka43tHD7yWCvjWhOcOR59K4nnDPDFeb5DL4oqL6I8hJb-5lvvhb3O3MOZKxLg5n7Tbb8eBgvZ2UQILnbcsq0x0b8WWtnLQtraLp0jBCOMahkMal7xOM5cQlPK5JkgMx6MqpQJQeQ-5KQN3KJmfbL9bT3YjjTyDautJ68Otn3K3-bVbJTMJjrIbtr5MbQH-UIsKRTrB2Q-5KL-3-QAeK3v5hJEyTK0eJ6bbtLeL2OOWfbdJJjofqrT5joBBpKhqfcu-fvRW2TxoUJ15DnJhhvG-R5a2J_ebPRiJPr9QgbPBhQ7tt5W8ncFbT7l5hKpbt-q0x-jLTnhVn0M5DK0HPonHjLaD6vX3J; rsv_jmp_slow=1712025189264; BA_HECTOR=a5a524ag2hak8k24058kaga0gkpgb01j0mrj61t; ZFY=Eb:AtLum72DaPORV9d5vbzyFa2T8c78oeqz:AqTp5bMsg:C



#DEPLOY_ENV=LOCAL/PROD
DEPLOY_ENV=PROD
#DB
LOCAL_DB_TYPE=mysql
LOCAL_DB_USER=dp_agggwapp
LOCAL_DB_PASSWORD=bRTfvwza61
LOCAL_DB_HOST=conf.data.yp
LOCAL_DB_PORT=5000
LOCAL_DB_NAME=dp_agggw

#REDIS
REDIS_HOST=common.codis.yp
REDIS_SK=woJgVGxXb3ZuW39USWw6YH9TdHhge28=

